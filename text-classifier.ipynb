{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41039d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee598ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\as\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\as\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import *\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")  \n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\", output_hidden_states=False)\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dacffc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'example', 'text', 'that', 'we', 'are', 'using', 'for', 'understanding', 'purpose', ',', 'another', 'word', 'that', 'we', 'gonna', 'use', 'is', 'ka', '##ggle']\n"
     ]
    }
   ],
   "source": [
    "sent = 'This is a example Text that we are using for Understanding Purpose, another word that we gonna use is Kaggle'\n",
    "tokens = bert_tokenizer.tokenize(sent)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "907db237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert_tokens_to_ids: [2023, 2003, 1037, 2742, 3793, 2008, 2057, 2024, 2478, 2005, 4824, 3800, 1010, 2178, 2773, 2008, 2057, 6069, 2224, 2003, 10556, 24679]\n",
      "\n",
      "\n",
      "encode: [101, 2023, 2003, 1037, 2742, 3793, 2008, 2057, 2024, 2478, 2005, 4824, 3800, 1010, 2178, 2773, 2008, 2057, 6069, 2224, 2003, 10556, 24679, 102, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "encode_plus: {'input_ids': [101, 2023, 2003, 1037, 2742, 3793, 2008, 2057, 2024, 2478, 2005, 4824, 3800, 1010, 2178, 2773, 2008, 2057, 6069, 2224, 2003, 10556, 24679, 102, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"convert_tokens_to_ids:\",ids)\n",
    "\n",
    "ids_encode = bert_tokenizer.encode(sent,add_special_tokens = True,max_length =30,pad_to_max_length = True,\n",
    "return_attention_mask = True)\n",
    "print(\"\\n\\nencode:\",ids_encode)\n",
    "\n",
    "ids_encode_plus = bert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =30,pad_to_max_length = True,\n",
    "return_attention_mask = True)\n",
    "print(\"\\n\\nencode_plus:\",ids_encode_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f87b3627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert_ids_to_tokens: ['this', 'is', 'a', 'example', 'text', 'that', 'we', 'are', 'using', 'for', 'understanding', 'purpose', ',', 'another', 'word', 'that', 'we', 'gonna', 'use', 'is', 'ka', '##ggle']\n",
      "\n",
      "decode: [CLS] this is a example text that we are using for understanding purpose, another word that we gonna use is kaggle [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(\"convert_ids_to_tokens:\",bert_tokenizer.convert_ids_to_tokens(ids))\n",
    "\n",
    "print(\"\\ndecode:\",bert_tokenizer.decode(ids_encode_plus['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48bb5ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "train = pd.read_csv('F://bbc-text.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22484b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            511\n",
       "business         510\n",
       "politics         417\n",
       "tech             401\n",
       "entertainment    386\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd14912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(temp):\n",
    "    temp = re.sub(\"@\\S+\", \" \", temp)\n",
    "    temp = re.sub(\"https*\\S+\", \" \", temp)\n",
    "    temp = re.sub(\"#\\S+\", \" \", temp)\n",
    "    temp = re.sub(\"\\'\\w+\", '', temp)\n",
    "    temp = re.sub(r'\\w*\\d+\\w*', '', temp)\n",
    "    temp = re.sub('\\s{2,}', \" \", temp)\n",
    "    \n",
    "    return temp.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b488e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text_clean'] = train['text'].apply(clean_text)\n",
    "sentences = train['text_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7743e7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2225/2225 [00:15<00:00, 144.69it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids=[]\n",
    "attention_masks=[]\n",
    "\n",
    "for sent in tqdm(sentences):\n",
    "    bert_inp = bert_tokenizer.encode_plus(sent,\n",
    "                                          add_special_tokens=True,\n",
    "                                          max_length =128,\n",
    "                                          pad_to_max_length=True,\n",
    "                                          return_attention_mask=True)\n",
    "    \n",
    "    input_ids.append(bert_inp['input_ids'])\n",
    "    attention_masks.append(bert_inp['attention_mask'])\n",
    "\n",
    "input_ids = np.asarray(input_ids)\n",
    "attention_masks = np.array(attention_masks)\n",
    "target = np.array(pd.get_dummies(train['category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ab129c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.convert_ids_to_tokens(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac2811bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, train_mask, test_mask = train_test_split(input_ids, target, attention_masks, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "973024f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,482,240\n",
      "Trainable params: 109,482,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "400f234c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           24608       ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 32)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 5)            165         ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,507,013\n",
      "Trainable params: 24,773\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(model_):\n",
    "    input_ids = tf.keras.Input(shape=(128,), dtype='int32')\n",
    "    attention_masks = tf.keras.Input(shape=(128,), dtype='int32')\n",
    "    \n",
    "    output = model_(input_ids, attention_masks)\n",
    "    output = output[0]      # this is inline in config.output_hidden_states as we want only the top head\n",
    "    \n",
    "    output = output[:,0,:]  #  We are only interested in <cls> or classification token of the model which can be extracted\n",
    "                            #  using the slice operation. Now we have 2D data and build the network as one desired.\n",
    "                            #  While converting 3D data to 2D we may miss on valuable info.\n",
    "    \n",
    "    output = tf.keras.layers.Dense(32, activation='relu')(output)\n",
    "    output = tf.keras.layers.Dropout(0.2)(output)\n",
    "    output = tf.keras.layers.Dense(5, activation='softmax')(output)\n",
    "    model = tf.keras.models.Model(inputs=[input_ids,attention_masks], outputs=output)\n",
    "    \n",
    "    \n",
    "    for layer in model.layers[:3]:\n",
    "        layer.trainable = False\n",
    "    return model\n",
    "\n",
    "model = create_model(bert_model)\n",
    "model.summary()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02860d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "14365ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98133e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "56/56 [==============================] - 285s 5s/step - loss: 0.9193 - accuracy: 0.6646 - val_loss: 0.2856 - val_accuracy: 0.9326\n",
      "Epoch 2/4\n",
      "56/56 [==============================] - 293s 5s/step - loss: 0.2927 - accuracy: 0.9140 - val_loss: 0.1957 - val_accuracy: 0.9461\n",
      "Epoch 3/4\n",
      "56/56 [==============================] - 314s 6s/step - loss: 0.2060 - accuracy: 0.9427 - val_loss: 0.1684 - val_accuracy: 0.9438\n",
      "Epoch 4/4\n",
      "56/56 [==============================] - 322s 6s/step - loss: 0.1630 - accuracy: 0.9539 - val_loss: 0.1521 - val_accuracy: 0.9506\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train, train_mask], y_train, batch_size=32, epochs=4, validation_data=([X_test, test_mask], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29376402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_3[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_bert_model[1][0]']          \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           24608       ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 32)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 5)            165         ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,507,013\n",
      "Trainable params: 24,773\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1c19c249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 150ms/step\n",
      "predictions: [[7.4171767e-02 2.2354107e-03 8.1301958e-04 9.0162307e-01 2.1156736e-02]]\n",
      "prediction is: sport\n"
     ]
    }
   ],
   "source": [
    "Ind2Label = {\n",
    "    0: 'politics',\n",
    "    1: 'entertainment',\n",
    "    2: 'business',\n",
    "    3: 'sport',\n",
    "    4: 'technology'\n",
    "}\n",
    "\n",
    "def predict(model, text):\n",
    "    bert_inputs = bert_tokenizer.encode_plus(text, max_length=128, padding='max_length', truncation=True)\n",
    "    input_ids = np.array([bert_inputs['input_ids']])\n",
    "    attention_mask = np.array([bert_inputs['attention_mask']])\n",
    "    predictions = model.predict([input_ids, attention_mask])\n",
    "    print(f\"predictions: {predictions}\")\n",
    "    predicted_index = np.argmax(predictions)\n",
    "    print(f\"prediction is: {Ind2Label[predicted_index]}\")\n",
    "    \n",
    "text = \"Serie A leaders Napoli moved 18 points clear at the top of the table thanks to a routine away victory over Sassuolo.\"\n",
    "predict(model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "97dd6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model=model, filepath='text-classifierr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b6756a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model('text-classifierr.h5', custom_objects={\"TFBertModel\": transformers.TFBertModel})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "456694b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "predictions: [[7.4171767e-02 2.2354107e-03 8.1301958e-04 9.0162307e-01 2.1156736e-02]]\n",
      "prediction is: sport\n"
     ]
    }
   ],
   "source": [
    "predict(loaded_model, text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fbd38b984bf10272edee72d0b9ab7dbd2af19c2df45979352ded5e14fa5543cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
